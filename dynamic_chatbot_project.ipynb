{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc96059d",
      "metadata": {
        "id": "fc96059d"
      },
      "source": [
        "1. Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b8ccdb",
      "metadata": {
        "id": "a0b8ccdb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string #punctuations, data preprocessing\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #data encoding\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer #5th step - data preprocessing\n",
        "import wikipedia\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6ce1865",
      "metadata": {
        "id": "d6ce1865"
      },
      "source": [
        "Function for tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46adfb3",
      "metadata": {
        "id": "d46adfb3"
      },
      "outputs": [],
      "source": [
        "def tokenize(user_query):\n",
        "    #file = open('corpus.txt','r',errors='ignore')\n",
        "    corpus = wikipedia.summary(user_query)\n",
        "    #corpus = file.read()\n",
        "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
        "    word_tokens = nltk.word_tokenize(corpus)\n",
        "    return sentence_tokens, word_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eefa12cb",
      "metadata": {
        "id": "eefa12cb"
      },
      "source": [
        "Function for Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9267142c",
      "metadata": {
        "id": "9267142c"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91f401d2",
      "metadata": {
        "id": "91f401d2"
      },
      "outputs": [],
      "source": [
        "def lemtokens(tokens):\n",
        "    lst=[]\n",
        "    for i in tokens: #every individual token have to be lemmatized\n",
        "        lst.append(lemmatizer.lemmatize(i))\n",
        "    return lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40cfa8a0",
      "metadata": {
        "id": "40cfa8a0"
      },
      "outputs": [],
      "source": [
        "#to remove the punctuation\n",
        "punct = dict((ord(i),None) for i in string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34103117",
      "metadata": {
        "id": "34103117"
      },
      "outputs": [],
      "source": [
        "def lemmer(text):\n",
        "    #Tokenize\n",
        "    tokenized_text=nltk.word_tokenize(text.lower().translate(punct))\n",
        "    lemmatized_values = lemtokens(tokenized_text)\n",
        "    return lemmatized_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e627921",
      "metadata": {
        "id": "6e627921"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8907a21d",
      "metadata": {
        "id": "8907a21d"
      },
      "source": [
        "Function for Greeting - Rule Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67916806",
      "metadata": {
        "id": "67916806"
      },
      "outputs": [],
      "source": [
        "greeting_inputs = ['hello', 'hi', 'hey', 'greetings', 'hii', 'what\\'s up', 'good morning', 'good afternoon', 'good evening', 'howdy', 'hey there', 'yo', 'hiya', 'good day', 'hiya', 'hey hey', 'hi there', 'how\\'s it going', 'nice to see you', 'how are you', 'what\\'s happening', 'how do you do', 'well hello', 'top of the morning', 'hey man', 'what\\'s new', 'long time no see', 'hi friend', 'hiya pal', 'good to see you', 'howdy partner', 'hey dude', 'what\\'s crackin\\'', 'hello there', 'hiya mate', 'hey buddy', 'g\\'day mate', 'greetings and salutations', 'hello my friend', 'yo!', 'hi there, stranger', 'well met', 'hey you!', 'hello again', 'hey you!', 'good to see you again', 'what\\'s cooking', 'good to meet you', 'how\\'s life', 'how\\'s everything', 'nice to meet you', 'how are things', 'how\\'s your day', 'hi, what\\'s up', 'how\\'s your day going', 'hey, how\\'s it going', 'how are you doing today', 'hey, how\\'s life', 'what\\'s new with you', 'how\\'ve you been', 'how\\'s everything going', 'how\\'s your day been so far', 'how\\'s your day going so far', 'how have you been', 'how\\'ve things been going', 'how\\'s life been treating you', 'how are things going', 'how are things with you', 'what\\'s been happening', 'how\\'ve you been doing', 'how are you getting on', 'how have you been doing', 'what\\'s been going on', 'how\\'ve you been keeping', 'how are you today', 'what\\'s going on with you', 'how\\'s everything been going', 'how are you getting along', 'how are you feeling', 'how have things been', 'how are you getting by', 'what have you been up to', 'how\\'s it been going', 'how are you', 'how\\'s things', 'what\\'s been up', 'how have you been', 'how are you coping', 'what\\'s been up with you', 'how are you holding up', 'how\\'s everything with you', 'how\\'s everything been', 'how are you faring', 'how have you been lately', 'how\\'s life treating you', 'how are you feeling today', 'how\\'s everything been with you', 'how\\'s life been', 'how are you managing', 'how are you doing', 'what\\'s going on', 'how are you today', 'what\\'s going on with you', 'what\\'s new', 'how are you feeling today', 'how are you today', 'what\\'s happening', 'how are you doing today', 'how are you', 'how\\'s it going', 'how are things', 'how are you feeling', 'how are you doing', 'how have you been', 'how are you getting on', 'how are you', 'how\\'s everything', 'how\\'ve you been', 'how are things', 'how are you getting by', 'how\\'s it going', 'how\\'s everything going', 'how are you doing', 'how\\'ve you been', 'how are you coping', 'how\\'ve things been going', 'how are you holding up', 'how are you feeling', 'how\\'s everything been', 'how are you faring', 'how have you been lately', 'how are you today', 'how\\'s everything been with you', 'how are you managing', 'how\\'s life been', 'how are you doing', 'how are you today', 'how are you feeling today', 'how are things going', 'how\\'s it been going', 'how\\'s life been treating you', 'how\\'s life treating you', 'how\\'s it going', 'how\\'s your day been', 'how are things with you', 'how\\'s your day', 'how have you been', 'how\\'s everything going', 'how\\'s your day been so far', 'how are you', 'how\\'s your day going so far', 'how\\'s your day been so far', 'how are things going', 'how are you doing', 'how are you today', 'how have you been', 'how are you feeling today', 'how are you doing today', 'how are things', 'how are you', 'how are you today', 'how have you been', 'how are you feeling today', 'how are you doing today', 'how have you been']\n",
        "greeting_responses = ['Hello I am a chatbot', 'hello', 'hi', 'whats up', 'hi there', 'hey!', 'nice to see you!', 'hey, how can I assist you?', 'hi, how are you?', 'hello, how can I help you today?', 'hey there! What can I do for you?', 'yo, what\\'s on your mind?', 'hiya!', 'hey, nice to meet you!', 'hello, nice to see you!', 'hi there, how\\'s it going?', 'hey, good to see you!', 'hi there, how can I help you?', 'hello, nice to meet you!', 'hiya! How can I assist you?', 'hey there! How can I help you?', 'hi there! How can I assist you today?', 'hi there! How may I help you?', 'hey! How can I assist you today?', 'hello! How may I assist you?', 'hi! What can I do for you?', 'hello! How can I help you?', 'hiya! How may I assist you?', 'hey! How may I help you today?', 'hello! What can I do for you today?', 'hi! How may I assist you today?', 'hello! How may I help you?', 'hi there! What can I assist you with?', 'hello there! How may I assist you?', 'hi there! How can I assist you?', 'hey there! How may I help you?', 'hi there! What can I do for you today?', 'hello there! How may I assist you today?', 'hi there! How can I help you today?', 'hello there! What can I assist you with?', 'hey there! How can I assist you today?', 'hi there! How can I help you?', 'hello! How may I assist you today?', 'hi! What can I do for you?', 'hello! How can I help you?', 'hiya! How may I assist you?', 'hey! How may I help you today?', 'hello! What can I do for you today?', 'hi! How may I assist you today?', 'hello! How may I help you?', 'hi there! What can I assist you with?', 'hello there! How may I assist you?', 'hi there! How can I assist you?', 'hey there! How may I help you?', 'hi there! What can I do for you today?', 'hello there! How may I assist you today?', 'hi there! How can I help you today?', 'hello there! What can I assist you with?', 'hey there! How can I assist you today?', 'hi there! How can I help you?', 'hello! How may I assist you today?', 'hi! What can I do for you?', 'hello! How can I help you?', 'hiya! How may I assist you?', 'hey! How may I help you today?', 'hello! What can I do for you today?', 'hi! How may I assist you today?', 'hello! How may I help you?', 'hi there! What can I assist you with?', 'hello there! How may I assist you?', 'hi there! How can I assist you?', 'hey there! How may I help you?', 'hi there! What can I do for you today?', 'hello there! How may I assist you today?', 'hi there! How can I help you today?', 'hello there! What can I assist you with?', 'hey there! How can I assist you today?', 'hi there! How can I help you?', 'hello! How may I assist you today?', 'hi! What can I do for you?', 'hello! How can I help you?', 'hiya! How may I assist you?', 'hey! How may I help you today?', 'hello! What can I do for you today?', 'hi! How may I assist you today?', 'hello! How may I help you?', 'hi there! What can I assist you with?', 'hello there! How may I assist you?', 'hi there! How can I assist you?', 'hey there! How may I help you?', 'hi there! What can I do for you today?', 'hello there! How may I assist you today?', 'hi there! How can I help you today?', 'hello there! What can I assist you with?', 'hey there! How can I assist you today?', 'hi there! How can I help you?', 'hello! How may I assist you today?', 'hi! What can I do for you?', 'hello! How can I help you?', 'hiya! How may I assist you?', 'hey! How may I help you today?', 'hello! What can I do for you today?', 'hi! How may I assist you today?', 'hello! How may I help you?', 'hi there! What can I assist you with?', 'hello there! How may I assist you?', 'hi there! How can I assist you?', 'hey there! How may I help you?', 'hi there! What can I do for you today?', 'hello there! How may I assist you today?', 'hi there! How can I help you today?', 'hello there! What can I assist you with?', 'hey there! How can I assist you today?', 'hi there! How can I help you?', 'hello! How may I assist you today?', 'hi! What can I do for you?', 'hello! How can I help you?', 'hiya! How may I assist you?', 'hey! How may I help you today?', 'hello! What can I do for you today?', 'hi! How may I assist you today?', 'hello! How may I help you?', 'hi there! What can I assist you with?', 'hello there! How may I assist you']\n",
        "\n",
        "def greeting(text):\n",
        "    #Tokenize\n",
        "    for token in text.split():\n",
        "        #inputs\n",
        "        if token.lower() in greeting_inputs:\n",
        "            return random.choice(greeting_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26970f21",
      "metadata": {
        "id": "26970f21"
      },
      "source": [
        "tfidf = [t1,t2,t3,t4,user_query]\n",
        "\n",
        "sim = [s1,s2,s3,s4,s5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "302a161c",
      "metadata": {
        "id": "302a161c"
      },
      "source": [
        "Function to generate response for the queries from the corpus!\n",
        "\n",
        "Data Encoding - TFIDF\n",
        "Similarity Metric - cosine similarity\n",
        "choosing the vector with maximum similarity in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b16e80",
      "metadata": {
        "id": "64b16e80"
      },
      "outputs": [],
      "source": [
        "def respond(user_query):\n",
        "    bot_response=''\n",
        "\n",
        "    #Tokenize\n",
        "    sent_tokens,word_tokens = tokenize(user_query)\n",
        "    sent_tokens.append(user_query)\n",
        "\n",
        "    #Vectorizing\n",
        "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer,stop_words=\"english\")\n",
        "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
        "\n",
        "    #cosine similarity\n",
        "    sim_values = cosine_similarity(tfidf[-1],tfidf) #cosine similarity of the last element with the entire list\n",
        "\n",
        "    #selecting the response or token with max similarity\n",
        "    index = sim_values.argsort()[0][-2]\n",
        "\n",
        "    flattened_sim = sim_values.flatten()\n",
        "    flattened_sim.sort()\n",
        "\n",
        "    required_tfidf = flattened_sim[-2]\n",
        "\n",
        "    if(required_tfidf==0):\n",
        "        bot_response += 'I cannot understand'\n",
        "        return bot_response\n",
        "    else:\n",
        "        bot_response += bot_response + sent_tokens[index]\n",
        "        return bot_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8846577a",
      "metadata": {
        "id": "8846577a"
      },
      "source": [
        "main() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6f8271",
      "metadata": {
        "id": "9f6f8271",
        "outputId": "08847d8c-3250-49e9-cb36-698dd8af1c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot\n",
            "hi\n",
            "Bot: hi\n",
            "apple fruit\n",
            "Bot:  An apple is an edible fruit produced by an apple tree (Malus domestica).\n",
            "orange fruit\n",
            "Bot:  As of 1987, orange trees were found to be the most cultivated fruit tree in the world.\n",
            "telescope\n",
            "Bot:  In the 20th century, many new types of telescopes were invented, including radio telescopes in the 1930s and infrared telescopes in the 1960s.\n",
            "laptop\n",
            "Bot:  Examples of specialized models of laptops include rugged notebooks for use in construction or military applications, as well as low production cost laptops such as those from the One Laptop per Child (OLPC) organization, which incorporate features like solar charging and semi-flexible components not found on most laptop computers.\n"
          ]
        }
      ],
      "source": [
        "print(\"Chatbot\")\n",
        "flag = 1\n",
        "\n",
        "while(flag == 1):\n",
        "    user_query = input()\n",
        "    user_query = user_query.lower()\n",
        "\n",
        "    #exit\n",
        "    if(user_query=='exit'):\n",
        "        flag=0\n",
        "        print('Bot: Bye C u!')\n",
        "\n",
        "    else:\n",
        "        #greeting\n",
        "        if(greeting(user_query) != None):\n",
        "            print(\"Bot: \"+greeting(user_query))\n",
        "\n",
        "        #general\n",
        "        else:\n",
        "            res = wikipedia.summary(user_query)\n",
        "            print(\"Bot: \",res)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}